{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics On Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The library \"metrics\" consist of Classification report and Confusion Matrics all terms\n",
    "# (Recall,Precision,Accuracy,Specificity etc)\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider three categories\n",
    "C = \"CAT\"\n",
    "D = \"DOG\"\n",
    "F = \"FOX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Precision for the cat class is the number of correctly predicted cat out of all the cat.\n",
    "* The Recall for cat is the number of correctly predicted cat photos out of the number of actual cat.\n",
    "* \"support means total actual positive for C, D, F resp.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1 1]\n",
      " [3 7 0]\n",
      " [6 2 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CAT      0.308     0.667     0.421         6\n",
      "         DOG      0.700     0.700     0.700        10\n",
      "         FOX      0.667     0.200     0.308        10\n",
      "\n",
      "    accuracy                          0.500        26\n",
      "   macro avg      0.558     0.522     0.476        26\n",
      "weighted avg      0.597     0.500     0.485        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# True_Values\n",
    "y_true = [C,C,C,C,C,C, F,F,F,F,F,F,F,F,F,F, D,D,D,D,D,D,D,D,D,D]\n",
    "\n",
    "# Predicted_Values\n",
    "y_pred = [C,C,C,C,D,F, C,C,C,C,C,C,D,D,F,F, C,C,C,D,D,D,D,D,D,D]\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Print the precision and recall, among other matrics\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro average(Macro average is the average of precision/recall/f1-score.) = (precision of class C + precision of class D + \n",
    "# precision of class F)/3 = (.308+.700+.667)/3 = 0.558, Similarly for recall and f1-score.\n",
    "\n",
    "# weighted average is precision of all classes merge together. \n",
    "# weighted average = ([precision of class C * Total case of cat] + [precision of class D * Total case of dog] + \n",
    "# [precision of class F * Total case of fox)/Total images of animals.\n",
    "# [[.308*6 + .700*10 + .667*10]/26 = .597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The F1 score i.e. the F1 score for the positive class in a binary classification model. And this is calculated as the \n",
    "# F1 = 2*(p*r)/(p+r).[looks for above dataset works well because its the balanced one]\n",
    "\n",
    "# The weighted F1 score is a special case where we report not only the score of positive class, but also the negative class. \n",
    "# This is important where we have imbalanced classes. Because the simple F1 score gives a good value even if our model predicts \n",
    "# positives all the times.\n",
    "\n",
    "# So the weighted average takes into account the number of samples of both the classes as well and can't be calculated by the \n",
    "# formula you mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Here am gonna consider both FPR and FNR as important because i don't want my model predict dog & fox most of the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
